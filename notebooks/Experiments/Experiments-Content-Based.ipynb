{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cce9cd1-faeb-4d72-afac-f0e276720d96",
   "metadata": {},
   "source": [
    "# Expirements - Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669be40-d740-4277-937a-422912e294ed",
   "metadata": {},
   "source": [
    "Recommends items similar to those a user liked, based on item/user attributes.\n",
    "\n",
    "Some papers:\n",
    "- [Survey on Collaborative Filtering, Content-based\n",
    "Filtering and Hybrid Recommendation System](https://www.academia.edu/download/59762468/10.1.1.695.642820190617-91457-z4s1rf.pdf)\n",
    "\n",
    "Table of content:\n",
    "- [NCF (xxx)]( #)\n",
    "\n",
    "Here are the different usable features:\n",
    "\n",
    "* **User Features**:\n",
    "    - Past interactions history\n",
    "    - User’s preferred tags, categories, or creators\n",
    "\n",
    "* **Item Features**:\n",
    "  - Video Duration\n",
    "  - Video Watch Ratio\n",
    "  - Captions / Tags / Categories\n",
    "  - Reports / Likes / Comments / \n",
    "\n",
    "* **Temporal Features**: Not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce263400-97c9-448a-8ab5-20adc79dcc1e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec48c12-7181-4de7-9b6f-a6ebc600bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:24:47.998810: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-05 14:24:48.119482: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-05 14:24:48.214548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746455088.316332     542 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746455088.344856     542 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746455088.545434     542 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746455088.545460     542 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746455088.545462     542 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746455088.545464     542 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-05 14:24:48.567139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "from cycler import cycler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "#import jieba\n",
    "import warnings\n",
    "\n",
    "from random_utils import set_seed\n",
    "from model_utils import normalize_ratings\n",
    "\n",
    "set_seed(45)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 13)\n",
    "colors = plt.get_cmap('tab10').colors\n",
    "plt.rc('axes', prop_cycle=cycler('color', colors))\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753759b-1f5c-4bcc-9287-f975271fd3a9",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ed3a44-ed0b-42e1-bd18-3ff9dc1b47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>4381</td>\n",
       "      <td>6067</td>\n",
       "      <td>2020-07-05 05:27:48.378</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>0.722103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>11635</td>\n",
       "      <td>6100</td>\n",
       "      <td>2020-07-05 05:28:00.057</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>1.907377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3649</td>\n",
       "      <td>22422</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 05:29:09.479</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>2.063311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5262</td>\n",
       "      <td>4479</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 05:30:43.285</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593898e+09</td>\n",
       "      <td>0.566388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8234</td>\n",
       "      <td>4602</td>\n",
       "      <td>11000</td>\n",
       "      <td>2020-07-05 05:35:43.459</td>\n",
       "      <td>20200705.0</td>\n",
       "      <td>1.593899e+09</td>\n",
       "      <td>0.418364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676565</th>\n",
       "      <td>7162</td>\n",
       "      <td>2267</td>\n",
       "      <td>11908</td>\n",
       "      <td>5467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.178160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676566</th>\n",
       "      <td>7162</td>\n",
       "      <td>2065</td>\n",
       "      <td>11919</td>\n",
       "      <td>6067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.964562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676567</th>\n",
       "      <td>7162</td>\n",
       "      <td>1296</td>\n",
       "      <td>16690</td>\n",
       "      <td>19870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.839960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676568</th>\n",
       "      <td>7162</td>\n",
       "      <td>4822</td>\n",
       "      <td>11862</td>\n",
       "      <td>24400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.486148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676569</th>\n",
       "      <td>7162</td>\n",
       "      <td>4364</td>\n",
       "      <td>2182</td>\n",
       "      <td>19367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4676570 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  video_id  play_duration  video_duration  \\\n",
       "0             14       148           4381            6067   \n",
       "1             14       183          11635            6100   \n",
       "2             14      3649          22422           10867   \n",
       "3             14      5262           4479            7908   \n",
       "4             14      8234           4602           11000   \n",
       "...          ...       ...            ...             ...   \n",
       "4676565     7162      2267          11908            5467   \n",
       "4676566     7162      2065          11919            6067   \n",
       "4676567     7162      1296          16690           19870   \n",
       "4676568     7162      4822          11862           24400   \n",
       "4676569     7162      4364           2182           19367   \n",
       "\n",
       "                            time        date     timestamp  watch_ratio  \n",
       "0        2020-07-05 05:27:48.378  20200705.0  1.593898e+09     0.722103  \n",
       "1        2020-07-05 05:28:00.057  20200705.0  1.593898e+09     1.907377  \n",
       "2        2020-07-05 05:29:09.479  20200705.0  1.593898e+09     2.063311  \n",
       "3        2020-07-05 05:30:43.285  20200705.0  1.593898e+09     0.566388  \n",
       "4        2020-07-05 05:35:43.459  20200705.0  1.593899e+09     0.418364  \n",
       "...                          ...         ...           ...          ...  \n",
       "4676565                      NaN         NaN           NaN     2.178160  \n",
       "4676566                      NaN         NaN           NaN     1.964562  \n",
       "4676567                      NaN         NaN           NaN     0.839960  \n",
       "4676568                      NaN         NaN           NaN     0.486148  \n",
       "4676569                      NaN         NaN           NaN     0.112666  \n",
       "\n",
       "[4676570 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_test = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/small_matrix.csv\")\n",
    "interactions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e08a314-35fe-445e-b9d7-e05cf621fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3649</td>\n",
       "      <td>13838</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 00:08:23.438</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.273397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9598</td>\n",
       "      <td>13665</td>\n",
       "      <td>10984</td>\n",
       "      <td>2020-07-05 00:13:41.297</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.244082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5262</td>\n",
       "      <td>851</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 00:16:06.687</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>0.107613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>862</td>\n",
       "      <td>9590</td>\n",
       "      <td>2020-07-05 00:20:26.792</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593880e+09</td>\n",
       "      <td>0.089885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8234</td>\n",
       "      <td>858</td>\n",
       "      <td>11000</td>\n",
       "      <td>2020-07-05 00:43:05.128</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593881e+09</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530801</th>\n",
       "      <td>7175</td>\n",
       "      <td>1281</td>\n",
       "      <td>34618</td>\n",
       "      <td>140017</td>\n",
       "      <td>2020-09-05 15:07:10.576</td>\n",
       "      <td>20200905</td>\n",
       "      <td>1.599290e+09</td>\n",
       "      <td>0.247241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530802</th>\n",
       "      <td>7175</td>\n",
       "      <td>3407</td>\n",
       "      <td>12619</td>\n",
       "      <td>21888</td>\n",
       "      <td>2020-09-05 15:08:45.228</td>\n",
       "      <td>20200905</td>\n",
       "      <td>1.599290e+09</td>\n",
       "      <td>0.576526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530803</th>\n",
       "      <td>7175</td>\n",
       "      <td>10360</td>\n",
       "      <td>2407</td>\n",
       "      <td>7067</td>\n",
       "      <td>2020-09-05 19:10:29.041</td>\n",
       "      <td>20200905</td>\n",
       "      <td>1.599304e+09</td>\n",
       "      <td>0.340597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530804</th>\n",
       "      <td>7175</td>\n",
       "      <td>10360</td>\n",
       "      <td>6455</td>\n",
       "      <td>7067</td>\n",
       "      <td>2020-09-05 19:10:36.995</td>\n",
       "      <td>20200905</td>\n",
       "      <td>1.599304e+09</td>\n",
       "      <td>0.913400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530805</th>\n",
       "      <td>7175</td>\n",
       "      <td>10389</td>\n",
       "      <td>12263</td>\n",
       "      <td>14304</td>\n",
       "      <td>2020-09-05 21:13:51.419</td>\n",
       "      <td>20200905</td>\n",
       "      <td>1.599312e+09</td>\n",
       "      <td>0.857313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12530806 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  video_id  play_duration  video_duration  \\\n",
       "0               0      3649          13838           10867   \n",
       "1               0      9598          13665           10984   \n",
       "2               0      5262            851            7908   \n",
       "3               0      1963            862            9590   \n",
       "4               0      8234            858           11000   \n",
       "...           ...       ...            ...             ...   \n",
       "12530801     7175      1281          34618          140017   \n",
       "12530802     7175      3407          12619           21888   \n",
       "12530803     7175     10360           2407            7067   \n",
       "12530804     7175     10360           6455            7067   \n",
       "12530805     7175     10389          12263           14304   \n",
       "\n",
       "                             time      date     timestamp  watch_ratio  \n",
       "0         2020-07-05 00:08:23.438  20200705  1.593879e+09     1.273397  \n",
       "1         2020-07-05 00:13:41.297  20200705  1.593879e+09     1.244082  \n",
       "2         2020-07-05 00:16:06.687  20200705  1.593879e+09     0.107613  \n",
       "3         2020-07-05 00:20:26.792  20200705  1.593880e+09     0.089885  \n",
       "4         2020-07-05 00:43:05.128  20200705  1.593881e+09     0.078000  \n",
       "...                           ...       ...           ...          ...  \n",
       "12530801  2020-09-05 15:07:10.576  20200905  1.599290e+09     0.247241  \n",
       "12530802  2020-09-05 15:08:45.228  20200905  1.599290e+09     0.576526  \n",
       "12530803  2020-09-05 19:10:29.041  20200905  1.599304e+09     0.340597  \n",
       "12530804  2020-09-05 19:10:36.995  20200905  1.599304e+09     0.913400  \n",
       "12530805  2020-09-05 21:13:51.419  20200905  1.599312e+09     0.857313  \n",
       "\n",
       "[12530806 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_train = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/big_matrix.csv\")\n",
    "interactions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adf23f4-4f00-47b9-ad42-973401b29dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>manual_cover_text</th>\n",
       "      <th>caption</th>\n",
       "      <th>topic_tag</th>\n",
       "      <th>first_level_category_id</th>\n",
       "      <th>first_level_category_name</th>\n",
       "      <th>second_level_category_id</th>\n",
       "      <th>second_level_category_name</th>\n",
       "      <th>third_level_category_id</th>\n",
       "      <th>third_level_category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>精神小伙路难走 程哥你狗粮慢点撒</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>颜值</td>\n",
       "      <td>673</td>\n",
       "      <td>颜值随拍</td>\n",
       "      <td>-124</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>27</td>\n",
       "      <td>高新数码</td>\n",
       "      <td>-124</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>-124</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>晚饭后，运动一下！</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "      <td>喜剧</td>\n",
       "      <td>727</td>\n",
       "      <td>搞笑互动</td>\n",
       "      <td>-124</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>我平淡无奇，惊艳不了时光，温柔不了岁月，我只想漫无目的的走走，努力发笔小财，给自己买花 自己长大.</td>\n",
       "      <td>[]</td>\n",
       "      <td>26</td>\n",
       "      <td>摄影</td>\n",
       "      <td>686</td>\n",
       "      <td>主题摄影</td>\n",
       "      <td>2434</td>\n",
       "      <td>景物摄影</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>五爱街最美美女 一天1q</td>\n",
       "      <td>#搞笑 #感谢快手我要上热门 #五爱市场 这真是完美搭配啊！</td>\n",
       "      <td>[五爱市场,感谢快手我要上热门,搞笑]</td>\n",
       "      <td>5</td>\n",
       "      <td>时尚</td>\n",
       "      <td>737</td>\n",
       "      <td>营销售卖</td>\n",
       "      <td>2596</td>\n",
       "      <td>女装</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id manual_cover_text  \\\n",
       "0         0           UNKNOWN   \n",
       "1         1           UNKNOWN   \n",
       "2         2           UNKNOWN   \n",
       "3         3           UNKNOWN   \n",
       "4         4      五爱街最美美女 一天1q   \n",
       "\n",
       "                                             caption            topic_tag  \\\n",
       "0                                   精神小伙路难走 程哥你狗粮慢点撒                   []   \n",
       "1                                                NaN                   []   \n",
       "2                                          晚饭后，运动一下！                   []   \n",
       "3  我平淡无奇，惊艳不了时光，温柔不了岁月，我只想漫无目的的走走，努力发笔小财，给自己买花 自己长大.                   []   \n",
       "4                     #搞笑 #感谢快手我要上热门 #五爱市场 这真是完美搭配啊！  [五爱市场,感谢快手我要上热门,搞笑]   \n",
       "\n",
       "   first_level_category_id first_level_category_name  \\\n",
       "0                        8                        颜值   \n",
       "1                       27                      高新数码   \n",
       "2                        9                        喜剧   \n",
       "3                       26                        摄影   \n",
       "4                        5                        时尚   \n",
       "\n",
       "   second_level_category_id second_level_category_name  \\\n",
       "0                       673                       颜值随拍   \n",
       "1                      -124                    UNKNOWN   \n",
       "2                       727                       搞笑互动   \n",
       "3                       686                       主题摄影   \n",
       "4                       737                       营销售卖   \n",
       "\n",
       "   third_level_category_id third_level_category_name  \n",
       "0                     -124                   UNKNOWN  \n",
       "1                     -124                   UNKNOWN  \n",
       "2                     -124                   UNKNOWN  \n",
       "3                     2434                      景物摄影  \n",
       "4                     2596                        女装  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_features = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/kuairec_caption_category.csv\", lineterminator='\\n')\n",
    "video_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5882511-1ee9-43eb-88f3-21293fc9c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[27, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id     feat\n",
       "0         0      [8]\n",
       "1         1  [27, 9]\n",
       "2         2      [9]\n",
       "3         3     [26]\n",
       "4         4      [5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_categories = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/item_categories.csv\")\n",
    "video_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08f395a-1de5-479c-a814-ef71e4cde2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'date', 'author_id', 'video_type', 'upload_dt',\n",
       "       'upload_type', 'visible_status', 'video_duration', 'video_width',\n",
       "       'video_height', 'music_id', 'video_tag_id', 'video_tag_name',\n",
       "       'show_cnt', 'show_user_num', 'play_cnt', 'play_user_num',\n",
       "       'play_duration', 'complete_play_cnt', 'complete_play_user_num',\n",
       "       'valid_play_cnt', 'valid_play_user_num', 'long_time_play_cnt',\n",
       "       'long_time_play_user_num', 'short_time_play_cnt',\n",
       "       'short_time_play_user_num', 'play_progress', 'comment_stay_duration',\n",
       "       'like_cnt', 'like_user_num', 'click_like_cnt', 'double_click_cnt',\n",
       "       'cancel_like_cnt', 'cancel_like_user_num', 'comment_cnt',\n",
       "       'comment_user_num', 'direct_comment_cnt', 'reply_comment_cnt',\n",
       "       'delete_comment_cnt', 'delete_comment_user_num', 'comment_like_cnt',\n",
       "       'comment_like_user_num', 'follow_cnt', 'follow_user_num',\n",
       "       'cancel_follow_cnt', 'cancel_follow_user_num', 'share_cnt',\n",
       "       'share_user_num', 'download_cnt', 'download_user_num', 'report_cnt',\n",
       "       'report_user_num', 'reduce_similar_cnt', 'reduce_similar_user_num',\n",
       "       'collect_cnt', 'collect_user_num', 'cancel_collect_cnt',\n",
       "       'cancel_collect_user_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_daily = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/item_daily_features.csv\")\n",
    "video_daily.head()\n",
    "video_daily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4ff426-6621-437c-9015-e68f8a802cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_active_degree</th>\n",
       "      <th>is_lowactive_period</th>\n",
       "      <th>is_live_streamer</th>\n",
       "      <th>is_video_author</th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>follow_user_num_range</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>fans_user_num_range</th>\n",
       "      <th>friend_user_num</th>\n",
       "      <th>...</th>\n",
       "      <th>onehot_feat8</th>\n",
       "      <th>onehot_feat9</th>\n",
       "      <th>onehot_feat10</th>\n",
       "      <th>onehot_feat11</th>\n",
       "      <th>onehot_feat12</th>\n",
       "      <th>onehot_feat13</th>\n",
       "      <th>onehot_feat14</th>\n",
       "      <th>onehot_feat15</th>\n",
       "      <th>onehot_feat16</th>\n",
       "      <th>onehot_feat17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>high_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>(0,10]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>184</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>(250,500]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>(10,50]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>(10,50]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>251</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>(100,150]</td>\n",
       "      <td>4</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>7171</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>(50,100]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>7172</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>(10,50]</td>\n",
       "      <td>2</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>7173</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>615</td>\n",
       "      <td>500+</td>\n",
       "      <td>3</td>\n",
       "      <td>[1,10)</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>7174</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>959</td>\n",
       "      <td>500+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>7175</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>(100,150]</td>\n",
       "      <td>35</td>\n",
       "      <td>[10,100)</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id user_active_degree  is_lowactive_period  is_live_streamer  \\\n",
       "0           0        high_active                    0                 0   \n",
       "1           1        full_active                    0                 0   \n",
       "2           2        full_active                    0                 0   \n",
       "3           3        full_active                    0                 0   \n",
       "4           4        full_active                    0                 0   \n",
       "...       ...                ...                  ...               ...   \n",
       "7171     7171        full_active                    0                 0   \n",
       "7172     7172        full_active                    0                 0   \n",
       "7173     7173        full_active                    0                 0   \n",
       "7174     7174        full_active                    0                 0   \n",
       "7175     7175        full_active                    0                 0   \n",
       "\n",
       "      is_video_author  follow_user_num follow_user_num_range  fans_user_num  \\\n",
       "0                   0                5                (0,10]              0   \n",
       "1                   0              386             (250,500]              4   \n",
       "2                   0               27               (10,50]              0   \n",
       "3                   0               16               (10,50]              0   \n",
       "4                   0              122             (100,150]              4   \n",
       "...               ...              ...                   ...            ...   \n",
       "7171                1               52              (50,100]              1   \n",
       "7172                0               45               (10,50]              2   \n",
       "7173                0              615                  500+              3   \n",
       "7174                0              959                  500+              0   \n",
       "7175                1               98             (100,150]             35   \n",
       "\n",
       "     fans_user_num_range  friend_user_num  ... onehot_feat8  onehot_feat9  \\\n",
       "0                      0                0  ...          184             6   \n",
       "1                 [1,10)                2  ...          186             6   \n",
       "2                      0                0  ...           51             2   \n",
       "3                      0                0  ...          251             3   \n",
       "4                 [1,10)                0  ...           99             4   \n",
       "...                  ...              ...  ...          ...           ...   \n",
       "7171              [1,10)                0  ...          259             1   \n",
       "7172              [1,10)                2  ...           11             2   \n",
       "7173              [1,10)                2  ...           51             2   \n",
       "7174                   0                0  ...          107             3   \n",
       "7175            [10,100)               33  ...          132             5   \n",
       "\n",
       "     onehot_feat10  onehot_feat11  onehot_feat12  onehot_feat13  \\\n",
       "0                3              0            0.0            0.0   \n",
       "1                2              0            0.0            0.0   \n",
       "2                3              0            0.0            0.0   \n",
       "3                2              0            0.0            0.0   \n",
       "4                2              0            0.0            0.0   \n",
       "...            ...            ...            ...            ...   \n",
       "7171             4              0            1.0            0.0   \n",
       "7172             0              0            1.0            0.0   \n",
       "7173             2              0            1.0            0.0   \n",
       "7174             2              0            0.0            0.0   \n",
       "7175             2              0            0.0            0.0   \n",
       "\n",
       "      onehot_feat14  onehot_feat15  onehot_feat16  onehot_feat17  \n",
       "0               0.0            0.0            0.0            0.0  \n",
       "1               0.0            0.0            0.0            0.0  \n",
       "2               0.0            0.0            0.0            0.0  \n",
       "3               0.0            0.0            0.0            0.0  \n",
       "4               0.0            0.0            0.0            0.0  \n",
       "...             ...            ...            ...            ...  \n",
       "7171            0.0            0.0            0.0            0.0  \n",
       "7172            0.0            0.0            0.0            0.0  \n",
       "7173            0.0            0.0            0.0            0.0  \n",
       "7174            0.0            0.0            0.0            0.0  \n",
       "7175            0.0            0.0            0.0            0.0  \n",
       "\n",
       "[7176 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features = pd.read_csv(\"../data_final_project/KuaiRec 2.0/data/user_features.csv\")\n",
    "user_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a4c1d-68e4-4237-b905-b76a391afedf",
   "metadata": {},
   "source": [
    "### XGBRanker\n",
    "\n",
    "Our first model will use a a XGBRanker that will try to maximize the NDCG on user and item representation from carefuly selected features including (see EDAs for more information):\n",
    "- Video features:\n",
    "    - **categorical**: upload_type (18 dummies), first video category (39 dummies)\n",
    "    - **text**: tags (TF-IDF), captions (TF-IDF)\n",
    "    - **numerical**: engagement metrics (like, comments, shares, reports and other ratios), video_duration, is_add\n",
    "- User features:\n",
    "    - **categorical**: is_live_streamer, is_user_full_active, is_live_streamer\n",
    "    - **numerical**: followers, fans, does video has its prefered category ?, does video has its prefered upload_type ?, video_duration_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103ce59-673c-42a6-824c-b19067886f78",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07fa6d-b353-4d31-99dc-afa8ad46a22b",
   "metadata": {},
   "source": [
    "##### Preprocessing on User\n",
    "\n",
    "First we preprocess the user features and videos features to create user representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca66468c-8a16-4cb6-b903-cec6721bb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_users(interactions: pd.DataFrame, user_features: pd.DataFrame, \n",
    "                    video_categories: pd.DataFrame, video_daily: pd.DataFrame) -> pd.DataFrame:\n",
    "    ###########\n",
    "    # Generic user stats\n",
    "    user_stats = pd.DataFrame({'user_id': user_features['user_id']})\n",
    "    user_stats['follower_fan_ratio'] = user_features['follow_user_num'] / (user_features['fans_user_num'] + 1)\n",
    "    user_stats['popularity_score'] = user_features['follow_user_num'] + user_features['fans_user_num']\n",
    "    user_stats['video_id_count'] = interactions.groupby('user_id').agg({'video_id': 'count'}).reset_index()['video_id']\n",
    "    user_stats['is_user_active'] = (user_features['user_active_degree'] == 'full_active').astype(int)\n",
    "\n",
    "    ###########\n",
    "    # Additional user features\n",
    "    user_features = user_features[['user_id', 'user_active_degree', 'is_lowactive_period', 'is_live_streamer', 'is_video_author']].copy()\n",
    "\n",
    "    ###########\n",
    "    # Get video information for user preference extraction\n",
    "    video_info = video_daily[['video_id', 'video_duration', 'upload_type', 'video_type']].drop_duplicates('video_id')\n",
    "    video_categories = video_categories.explode('feat').rename(columns={'feat': 'category_id'})\n",
    "    video_info = pd.merge(video_info, video_categories, on='video_id', how='left')\n",
    "\n",
    "    # Find user preferred categories and video types based on their interactions\n",
    "    user_video_interactions = pd.merge(interactions[['video_id', 'user_id', 'watch_ratio']], video_info, on='video_id', how='left')\n",
    "\n",
    "    # Get preferred categories\n",
    "    top_categories = (user_video_interactions\n",
    "        .groupby(['user_id', 'category_id'])\n",
    "        .agg({\n",
    "            'watch_ratio': 'sum',\n",
    "            'video_id': 'count'\n",
    "        })\n",
    "        .reset_index()\n",
    "        .sort_values(['user_id', 'watch_ratio'], ascending=[True, False])\n",
    "        .groupby('user_id')\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'category_id': 'preferred_category'})\n",
    "    )\n",
    "    top_categories['preferred_category'] = top_categories['preferred_category'].astype(str).str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "    # Get preferred upload types\n",
    "    top_upload_types = (user_video_interactions\n",
    "        .groupby(['user_id', 'upload_type'])\n",
    "        .agg({\n",
    "            'watch_ratio': 'sum',\n",
    "            'video_id': 'count'\n",
    "        })\n",
    "        .reset_index()\n",
    "        .sort_values(['user_id', 'watch_ratio'], ascending=[True, False])\n",
    "        .groupby('user_id')\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'upload_type': 'preferred_upload_type'})\n",
    "    )\n",
    "\n",
    "    # Get preferred video_type\n",
    "    top_video_types = (user_video_interactions\n",
    "        .groupby(['user_id', 'video_type'])\n",
    "        .agg({\n",
    "            'watch_ratio': 'sum',\n",
    "            'video_id': 'count'\n",
    "        })\n",
    "        .reset_index()\n",
    "        .sort_values(['user_id', 'watch_ratio'], ascending=[True, False])\n",
    "        .groupby('user_id')\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'video_type': 'preferred_video_type'})\n",
    "    )\n",
    "    \n",
    "    # Get preferred video duration (average of watched videos)\n",
    "    user_duration_prefs = (user_video_interactions\n",
    "        .groupby('user_id')\n",
    "        .agg({\n",
    "            'video_duration': 'mean'\n",
    "        })\n",
    "        .reset_index()\n",
    "        .rename(columns={'video_duration': 'preferred_duration'})\n",
    "    )\n",
    "\n",
    "    # Merge all user preference data\n",
    "    dfs = [\n",
    "        user_features,\n",
    "        user_stats,\n",
    "        top_categories[['user_id', 'preferred_category']],\n",
    "        top_video_types[['user_id', 'preferred_video_type']],\n",
    "        top_upload_types[['user_id', 'preferred_upload_type']],\n",
    "        user_duration_prefs\n",
    "    ]\n",
    "    users = reduce(lambda left, right: pd.merge(left, right, on='user_id', how='left'), dfs)\n",
    "\n",
    "    # Fill NaN values\n",
    "    fill_dict = {\n",
    "        'video_id_count': 0,\n",
    "        'follower_fan_ratio': 0,\n",
    "        'popularity_score': 0,\n",
    "        'preferred_category': -1,\n",
    "        'preferred_duration': users['preferred_duration'].median() if not users['preferred_duration'].isna().all() else 0\n",
    "    }\n",
    "    users.fillna(fill_dict, inplace=True)\n",
    "\n",
    "    print(f\"Processed {len(users)} unique users\")\n",
    "    return users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55905e91-23c3-4614-a723-ce9de1a17c01",
   "metadata": {},
   "source": [
    "##### Preprocessing on Videos\n",
    "\n",
    "We do the same thing for videos with generic information about the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e6a5f4-6925-4057-84f7-aeb2eee888b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess_videos(video_features: pd.DataFrame, video_daily: pd.DataFrame, \n",
    "                         video_categories: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Computing generic video stats and information\n",
    "    video_stats = video_daily.groupby('video_id').agg({\n",
    "        'play_cnt': 'mean',\n",
    "        'play_duration': 'mean',\n",
    "        'like_cnt': 'mean',\n",
    "        'cancel_like_cnt': 'mean',\n",
    "        'comment_cnt': 'mean',\n",
    "        #'reply_comment_cnt': 'mean',\n",
    "        'share_cnt': 'mean',\n",
    "        'download_cnt': 'mean',\n",
    "        'report_cnt': 'mean',\n",
    "        'follow_cnt': 'mean',\n",
    "        'cancel_follow_cnt': 'mean',\n",
    "        # first value for categorical/constant features\n",
    "        'video_duration': 'first', \n",
    "        'video_type': 'first',\n",
    "        #'video_tag_id': 'first',\n",
    "        #'video_tag_name': 'first',\n",
    "        #'author_id': 'first',\n",
    "        'upload_type': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Engagement metrics\n",
    "    video_stats['like_play_ratio'] = video_stats['like_cnt'] / (video_stats['play_cnt'] + 1)\n",
    "    video_stats['comment_play_ratio'] = video_stats['comment_cnt'] / (video_stats['play_cnt'] + 1)\n",
    "    video_stats['share_play_ratio'] = video_stats['share_cnt'] / (video_stats['play_cnt'] + 1)\n",
    "    video_stats['follow_play_ratio'] = video_stats['follow_cnt'] / (video_stats['play_cnt'] + 1)\n",
    "    video_stats['follow_cancel_ratio'] = video_stats['cancel_follow_cnt'] / (video_stats['follow_cnt'] + 1)\n",
    "    video_stats['like_cancel_ratio'] = video_stats['cancel_like_cnt'] / (video_stats['like_cnt'] + 1)\n",
    "    video_stats['like_to_comment_ratio'] = video_stats['like_cnt'] / (video_stats['comment_cnt'] + 1)\n",
    "\n",
    "    # Binarize cat\n",
    "    video_stats[\"is_add\"] = (video_stats[\"video_type\"] == \"AD\").astype(int)\n",
    "\n",
    "    # Keep all relevant columns\n",
    "    video_stats = video_stats[['video_id', 'like_play_ratio', 'comment_play_ratio', 'share_play_ratio',\n",
    "                'like_cancel_ratio', 'video_duration', 'is_add', 'like_to_comment_ratio', \n",
    "                'upload_type', 'follow_play_ratio', 'follow_cancel_ratio']]\n",
    "\n",
    "    # Join with video features and categories\n",
    "    videos = pd.merge(video_stats, video_features[['video_id', 'manual_cover_text', 'caption', 'topic_tag']], \n",
    "                     on='video_id', how='left').merge(video_categories, on='video_id', how='left')\n",
    "\n",
    "    # Process text features if needed\n",
    "    def parse_tags(tag_str):\n",
    "        if isinstance(tag_str, str):\n",
    "            tag_str = tag_str.strip(\"[]\")\n",
    "            tags = [tag.strip() for tag in tag_str.split(\",\") if tag.strip()]\n",
    "            return tags[:10]\n",
    "        return []\n",
    "\n",
    "    # Parse tags and concatenate with caption\n",
    "    videos['parsed_tags'] = videos['topic_tag'].apply(parse_tags)\n",
    "    videos['caption'] = videos['caption'].fillna('')\n",
    "    videos['manual_cover_text'] = videos['manual_cover_text'].apply(lambda x: '' if x == 'UNKNOWN' else x)\n",
    "    videos['tags_caption_cover'] = videos.apply(\n",
    "        lambda row: ' '.join(row['parsed_tags']) + ' ' + row['caption'] + ' ' + row['manual_cover_text'], axis=1\n",
    "    )\n",
    "    videos.drop(columns=['parsed_tags', 'caption', 'topic_tag', 'manual_cover_text'], inplace=True)\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32223597-3e84-410b-a2ea-ebf809490a3a",
   "metadata": {},
   "source": [
    "#### Full Preprocessing pipeline\n",
    "\n",
    "We implement the full pipeline with fit and transform steps after both user and video preprocessing steps are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1414e692-b56f-4f5a-8e5d-75e3ddc61ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingXGRanker:\n",
    "    def __init__(self, tfidf_max_features=64):\n",
    "        #  Representations, should be computed once by fit\n",
    "        self.users = None\n",
    "        self.videos = None\n",
    "        # Registering different features\n",
    "        self.cat_cols = ['upload_type']\n",
    "        self.binary_cols = ['is_add', 'category_match', 'upload_type_match', 'is_weekend', 'is_user_active',\n",
    "                            'is_lowactive_period', 'is_live_streamer', \t'is_video_author']\n",
    "        self.num_cols = ['duration_diff', 'like_play_ratio', 'comment_play_ratio',\n",
    "            'share_play_ratio', 'follow_play_ratio', 'follow_cancel_ratio',\n",
    "            'like_cancel_ratio', 'like_to_comment_ratio', 'popularity_score', 'follower_fan_ratio'\n",
    "        ]\n",
    "        self.target_col = \"engagement\"\n",
    "        # Some variables that should be reused in transform step\n",
    "        self.cat_values = {}\n",
    "        self.num_cols_medians = {}\n",
    "        self.cat_cols_modes = {}\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.tfidf_video = None\n",
    "        self.standard_scaler = None\n",
    "        self.ohe = None\n",
    "\n",
    "    def _generate_features(self, df: pd.DataFrame) -> None:\n",
    "        df['category_match'] = df.apply(lambda row: int(row['preferred_category'] in row['feat']), axis=1)\n",
    "        df['upload_type_match'] = (df['preferred_upload_type'] == df['upload_type']).astype(int)\n",
    "        #df['video_type_match'] = (df['preferred_video_type'] == df['video_type']).astype(int)\n",
    "        df['duration_diff'] = abs(df['video_duration'] - df['preferred_duration'])\n",
    "\n",
    "    def _merge_representations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_merged = df[['video_id', 'user_id', 'engagement']].merge(self.videos, on='video_id', how='left')\n",
    "        df_merged.dropna(subset=['video_id', 'user_id', 'engagement'], inplace=True)\n",
    "        return df_merged.merge(self.users, on='user_id', how='left').dropna(subset=['video_id', 'user_id', 'engagement'])\n",
    "\n",
    "    def _encode_categoricals(self, df: pd.DataFrame, is_fit: bool) -> None:\n",
    "        if is_fit:\n",
    "            self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "            encoded = self.ohe.fit_transform(df[self.cat_cols])\n",
    "        else:\n",
    "            encoded = self.ohe.transform(df[self.cat_cols])\n",
    "\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded.astype(np.int16),\n",
    "            columns=self.ohe.get_feature_names_out(self.cat_cols),\n",
    "            index=df.index\n",
    "        )\n",
    "\n",
    "        df.drop(columns=self.cat_cols, inplace=True)\n",
    "        df[encoded_df.columns] = encoded_df\n",
    "\n",
    "    def _drop_and_fill_missing(self, df: pd.DataFrame, is_fit: bool) -> None:\n",
    "        col_to_remove = (set(self.num_cols) | set(self.cat_cols) | set(self.binary_cols) | set([\"tags_caption_cover\"]) | set([self.target_col])) - set(df.columns)\n",
    "        print(f\"Removing columns: {col_to_remove}\")\n",
    "        df.drop(columns=list(col_to_remove), inplace=True, errors='ignore')\n",
    "\n",
    "        # NUMERIC\n",
    "        for col in self.num_cols:\n",
    "            if is_fit:\n",
    "                self.num_cols_medians[col] = df[col].median()\n",
    "            df[col].fillna(self.num_cols_medians.get(col, df[col].median()), inplace=True)\n",
    "\n",
    "        # CATEGORICAL\n",
    "        for col in self.cat_cols:\n",
    "            mode = self.cat_cols_modes.get(col) if not is_fit else (df[col].mode()[0] if not df[col].mode().empty else 'unknown')\n",
    "            if is_fit:\n",
    "                self.cat_cols_modes[col] = mode\n",
    "            df[col].fillna(mode, inplace=True)\n",
    "\n",
    "    def _scale_numerical(self, df: pd.DataFrame, is_fit: bool) -> None:\n",
    "        if is_fit:\n",
    "            self.standard_scaler = StandardScaler()\n",
    "            df[self.num_cols] = self.standard_scaler.fit_transform(df[self.num_cols])\n",
    "        else:\n",
    "            df[self.num_cols] = self.standard_scaler.transform(df[self.num_cols])\n",
    "\n",
    "    def transform(self, interactions: pd.DataFrame, video_features: pd.DataFrame, video_daily: pd.DataFrame, \n",
    "                  video_categories: pd.DataFrame, user_features: pd.DataFrame, save = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        if self.users is None or self.videos is None or self.standard_scaler is None or self.ohe is None:\n",
    "            raise ValueError(\"Preprocessor has not been fitted. Call fit_transform first.\")\n",
    "\n",
    "        interactions_clean = interactions.copy()\n",
    "\n",
    "        print(\"Processing interaction data...\")\n",
    "        interactions_clean.drop_duplicates(['user_id', 'video_id'], keep='first', inplace=True)\n",
    "        interactions_clean['engagement'] = interactions_clean['watch_ratio'] * (\n",
    "            np.log1p(interactions_clean['play_duration']) / np.log1p(interactions_clean['video_duration'])\n",
    "        )\n",
    "        #interactions_clean['time'] = pd.to_datetime(interactions_clean['timestamp'], unit='s')\n",
    "        #interactions_clean['is_weekend'] = (interactions_clean['time'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "        print(\"Combining all features...\")\n",
    "        df_interactions = self._merge_representations(interactions_clean)\n",
    "\n",
    "        print(\"Generating new features...\")\n",
    "        self._generate_features(df_interactions)\n",
    "\n",
    "        print(\"Handling missing features (cat / num)...\")\n",
    "        self._drop_and_fill_missing(df_interactions, is_fit=False)\n",
    "\n",
    "        print(\"Encoding Categorical features...\")\n",
    "        self._encode_categoricals(df_interactions, is_fit=False)\n",
    "\n",
    "        print(\"Scaling Numerical features...\")\n",
    "        self._scale_numerical(df_interactions, is_fit=False)\n",
    "\n",
    "        #print(\"TF-IDF on text\")\n",
    "        #tfidf_matrix = self.tfidf_video.fit_transform(df_interactions['tags_caption_cover'])\n",
    "        #tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[\n",
    "        #    f'tfidf_feature_{i}' for i in range(tfidf_matrix.shape[1])\n",
    "        #])\n",
    "        #df_interactions = pd.concat([df_interactions.reset_index(drop=True), tfidf_df], axis=1)\n",
    "        #df_interactions.drop(columns=['tags_caption_cover'], inplace=True)\n",
    "\n",
    "        df_interactions.drop(columns=['preferred_category', 'preferred_video_type', 'preferred_upload_type', 'feat', 'tags_caption_cover'], inplace=True)        \n",
    "        print(f\"Final dataset shape: {df_interactions.shape}\")\n",
    "        print(df_interactions.columns)\n",
    "\n",
    "        if save:\n",
    "            df_interactions.to_parquet(\"dataset/test_processed_data.parquet\", compression='gzip')\n",
    "\n",
    "        return df_interactions\n",
    "    \n",
    "    def fit_transform(self, interactions: pd.DataFrame, video_features: pd.DataFrame, video_daily: pd.DataFrame, \n",
    "                   video_categories: pd.DataFrame, user_features: pd.DataFrame, save = True) -> tuple:\n",
    "        interactions_clean = interactions.drop_duplicates(['user_id', 'video_id'], keep='first')\n",
    "    \n",
    "        print(\"Processing interaction data...\")\n",
    "        interactions_clean['engagement'] = interactions_clean['watch_ratio'] * (\n",
    "            np.log1p(interactions_clean['play_duration']) / np.log1p(interactions_clean['video_duration'])\n",
    "        )\n",
    "        interactions_clean['time'] = pd.to_datetime(interactions_clean['timestamp'], unit='s')\n",
    "        interactions_clean['is_weekend'] = interactions_clean['time'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "        print(\"Processing videos...\")\n",
    "        self.videos = preprocess_videos(video_features, video_daily, video_categories)\n",
    "        print(self.videos.head())\n",
    "\n",
    "        print(\"Processing users...\")\n",
    "        self.users = preprocess_users(interactions_clean, user_features, video_categories, video_daily)\n",
    "        print(self.users.head())\n",
    "    \n",
    "        print(\"Combining all features...\")\n",
    "        df_interactions = self._merge_representations(interactions_clean)\n",
    "\n",
    "        del interactions_clean\n",
    "\n",
    "        print(\"Generating new features...\")\n",
    "        self._generate_features(df_interactions)\n",
    "\n",
    "        print(\"Dropping features...\")\n",
    "        self._drop_and_fill_missing(df_interactions, is_fit=True)\n",
    "\n",
    "        print(\"Encoding Categoricals...\")\n",
    "        self._encode_categoricals(df_interactions, is_fit=True)\n",
    "\n",
    "        print(\"Scaling numerical...\")\n",
    "        self._scale_numerical(df_interactions, is_fit=True)\n",
    "\n",
    "        #print(\"TF-IDF on Text..\")\n",
    "        #self.tfidf_video = TfidfVectorizer(max_features=self.tfidf_max_features)\n",
    "        #tfidf_matrix = self.tfidf_video.fit_transform(df_interactions['tags_caption_cover'])\n",
    "        #tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[\n",
    "        #    f'tfidf_feature_{i}' for i in range(tfidf_matrix.shape[1])\n",
    "        #])\n",
    "        #df_interactions = pd.concat([df_interactions.reset_index(drop=True), tfidf_df], axis=1)\n",
    "        #df_interactions.drop(columns=['tags_caption_cover'], inplace=True)\n",
    "\n",
    "        for col in df_interactions.select_dtypes(include=['int', 'int64', 'int32']).columns:\n",
    "            df_interactions[col] = df_interactions[col].astype(np.int16)\n",
    "        for col in df_interactions.select_dtypes(include=['float', 'float64']).columns:\n",
    "            df_interactions[col] = df_interactions[col].astype(np.float32)\n",
    "\n",
    "        df_interactions.drop(columns=['preferred_category', 'preferred_video_type', 'preferred_upload_type', 'feat', 'tags_caption_cover', 'user_active_degree'], inplace=True)        \n",
    "        print(f\"Final dataset shape: {df_interactions.shape}\")\n",
    "        print(df_interactions.columns)\n",
    "\n",
    "        #df_interactions.to_csv(\"dataset/df_train_interactions.csv\", index=False, chunksize=100000)\n",
    "        if save:\n",
    "            print(\"Saving train dataframe into parquet: dataset/train_processed_data.parquet\")\n",
    "            df_interactions.to_parquet(\"dataset/train_processed_data.parquet\", compression='gzip')\n",
    "        return df_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f269f3-05d3-4aac-9983-a07d9856db2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interaction data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115/556818873.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interactions_clean['engagement'] = interactions_clean['watch_ratio'] * (\n",
      "/tmp/ipykernel_115/556818873.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interactions_clean['time'] = pd.to_datetime(interactions_clean['timestamp'], unit='s')\n",
      "/tmp/ipykernel_115/556818873.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interactions_clean['is_weekend'] = interactions_clean['time'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos...\n",
      "   video_id  like_play_ratio  comment_play_ratio  share_play_ratio  \\\n",
      "0         0         0.059485            0.001088          0.000255   \n",
      "1         1         0.058329            0.000416          0.000452   \n",
      "2         2         0.004744            0.000045          0.000028   \n",
      "3         3         0.102283            0.000761          0.002826   \n",
      "4         4         0.004532            0.000000          0.000000   \n",
      "\n",
      "   like_cancel_ratio  video_duration  is_add  like_to_comment_ratio  \\\n",
      "0           0.187164          5966.0       0              47.931507   \n",
      "1           0.603601             NaN       0              67.762295   \n",
      "2           0.117484          8000.0       0              34.193548   \n",
      "3           0.147410             NaN       0              13.442857   \n",
      "4           0.090909         18000.0       0               0.057692   \n",
      "\n",
      "   upload_type  follow_play_ratio  follow_cancel_ratio     feat  \\\n",
      "0  ShortImport           0.030664                  0.0      [8]   \n",
      "1   PictureSet           0.001736                  0.0  [27, 9]   \n",
      "2       Kmovie           0.000603                  0.0      [9]   \n",
      "3   PictureSet           0.004022                  0.0     [26]   \n",
      "4  ShortCamera           0.001511                  0.0      [5]   \n",
      "\n",
      "                                  tags_caption_cover  \n",
      "0                                  精神小伙路难走 程哥你狗粮慢点撒   \n",
      "1                                                     \n",
      "2                                         晚饭后，运动一下！   \n",
      "3   我平淡无奇，惊艳不了时光，温柔不了岁月，我只想漫无目的的走走，努力发笔小财，给自己买花 自...  \n",
      "4  五爱市场 感谢快手我要上热门 搞笑 #搞笑 #感谢快手我要上热门 #五爱市场 这真是完美搭配...  \n",
      "Processing users...\n",
      "Processed 7176 unique users\n",
      "   user_id user_active_degree  is_lowactive_period  is_live_streamer  \\\n",
      "0        0        high_active                    0                 0   \n",
      "1        1        full_active                    0                 0   \n",
      "2        2        full_active                    0                 0   \n",
      "3        3        full_active                    0                 0   \n",
      "4        4        full_active                    0                 0   \n",
      "\n",
      "   is_video_author  follower_fan_ratio  popularity_score  video_id_count  \\\n",
      "0                0                 5.0                 5            2190   \n",
      "1                0                77.2               390            1297   \n",
      "2                0                27.0                27             976   \n",
      "3                0                16.0                16            2784   \n",
      "4                0                24.4               126             511   \n",
      "\n",
      "   is_user_active preferred_category preferred_video_type  \\\n",
      "0               0                 28               NORMAL   \n",
      "1               1                 28               NORMAL   \n",
      "2               1                 12               NORMAL   \n",
      "3               1                 28               NORMAL   \n",
      "4               1                 26               NORMAL   \n",
      "\n",
      "  preferred_upload_type  preferred_duration  \n",
      "0           ShortImport        10686.049807  \n",
      "1           ShortImport        14188.870515  \n",
      "2           ShortImport        14634.613071  \n",
      "3           ShortImport        12005.558015  \n",
      "4           ShortImport        14508.349272  \n",
      "Combining all features...\n",
      "Generating new features...\n",
      "Dropping features...\n",
      "Removing columns: {'is_weekend'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115/556818873.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(self.num_cols_medians.get(col, df[col].median()), inplace=True)\n",
      "/tmp/ipykernel_115/556818873.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Categoricals...\n",
      "Scaling numerical...\n",
      "Final dataset shape: (10300969, 41)\n",
      "Index(['video_id', 'user_id', 'engagement', 'like_play_ratio',\n",
      "       'comment_play_ratio', 'share_play_ratio', 'like_cancel_ratio',\n",
      "       'video_duration', 'is_add', 'like_to_comment_ratio',\n",
      "       'follow_play_ratio', 'follow_cancel_ratio', 'is_lowactive_period',\n",
      "       'is_live_streamer', 'is_video_author', 'follower_fan_ratio',\n",
      "       'popularity_score', 'video_id_count', 'is_user_active',\n",
      "       'preferred_duration', 'category_match', 'upload_type_match',\n",
      "       'duration_diff', 'upload_type_AiCutVideo', 'upload_type_FlashPhoto',\n",
      "       'upload_type_FollowShoot', 'upload_type_Kmovie',\n",
      "       'upload_type_LocalCollection', 'upload_type_LocalIntelligenceAlbum',\n",
      "       'upload_type_LongCamera', 'upload_type_LongImport',\n",
      "       'upload_type_LongPicture', 'upload_type_PhotoCopy',\n",
      "       'upload_type_PictureCopy', 'upload_type_PictureSet',\n",
      "       'upload_type_SameFrame', 'upload_type_ShareFromOtherApp',\n",
      "       'upload_type_ShortCamera', 'upload_type_ShortImport',\n",
      "       'upload_type_UNKNOWN', 'upload_type_Web'],\n",
      "      dtype='object')\n",
      "Saving train dataframe into parquet: dataset/train_processed_data.parquet\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingXGRanker(tfidf_max_features=32)\n",
    "\n",
    "df_train = pipeline.fit_transform(\n",
    "    interactions=interactions_train,\n",
    "    video_features=video_features,\n",
    "    video_daily=video_daily,\n",
    "    video_categories=video_categories,\n",
    "    user_features=user_features,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd153bb-9d84-4bf9-bdea-8f9764e5b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interaction data...\n",
      "Combining all features...\n",
      "Generating new features...\n",
      "Handling missing features (cat / num)...\n",
      "Removing columns: {'is_weekend'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115/556818873.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(self.num_cols_medians.get(col, df[col].median()), inplace=True)\n",
      "/tmp/ipykernel_115/556818873.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Categorical features...\n",
      "Scaling Numerical features...\n",
      "Final dataset shape: (4676570, 42)\n",
      "Index(['video_id', 'user_id', 'engagement', 'like_play_ratio',\n",
      "       'comment_play_ratio', 'share_play_ratio', 'like_cancel_ratio',\n",
      "       'video_duration', 'is_add', 'like_to_comment_ratio',\n",
      "       'follow_play_ratio', 'follow_cancel_ratio', 'user_active_degree',\n",
      "       'is_lowactive_period', 'is_live_streamer', 'is_video_author',\n",
      "       'follower_fan_ratio', 'popularity_score', 'video_id_count',\n",
      "       'is_user_active', 'preferred_duration', 'category_match',\n",
      "       'upload_type_match', 'duration_diff', 'upload_type_AiCutVideo',\n",
      "       'upload_type_FlashPhoto', 'upload_type_FollowShoot',\n",
      "       'upload_type_Kmovie', 'upload_type_LocalCollection',\n",
      "       'upload_type_LocalIntelligenceAlbum', 'upload_type_LongCamera',\n",
      "       'upload_type_LongImport', 'upload_type_LongPicture',\n",
      "       'upload_type_PhotoCopy', 'upload_type_PictureCopy',\n",
      "       'upload_type_PictureSet', 'upload_type_SameFrame',\n",
      "       'upload_type_ShareFromOtherApp', 'upload_type_ShortCamera',\n",
      "       'upload_type_ShortImport', 'upload_type_UNKNOWN', 'upload_type_Web'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_test = pipeline.transform(\n",
    "    interactions=interactions_test,\n",
    "    video_features=video_features,\n",
    "    video_daily=video_daily,\n",
    "    video_categories=video_categories,\n",
    "    user_features=user_features,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9a82f-6032-40cd-9a28-bbb73bfc813c",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723c81cd-3c2e-4de3-bee8-c265529b006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedFilteringXGRanker:\n",
    "    \"\"\"\n",
    "    Content-based filtering ranker using XGBoost with K-Fold cross-validation\n",
    "    and comprehensive logging capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.models = []\n",
    "        self.best_model = None\n",
    "        self.feature_importances = None\n",
    "        self.verbose = verbose\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO if self.verbose else logging.WARNING,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger('XGRanker')\n",
    "        \n",
    "    def _create_dmatrix(self, X: np.ndarray, y: Optional[np.ndarray] = None, \n",
    "                        groups: Optional[np.ndarray] = None) -> xgb.DMatrix:\n",
    "        dmatrix = xgb.DMatrix(X, y)\n",
    "        if groups is not None:\n",
    "            unique_groups, group_counts = np.unique(groups, return_counts=True)\n",
    "            dmatrix.set_group(group_counts)\n",
    "        return dmatrix\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, groups: np.ndarray, \n",
    "            params: Optional[Dict[str, Any]] = None, n_folds: int = 3, \n",
    "            early_stopping_rounds: int = 50):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Default parameters if none provided\n",
    "        if params is None:\n",
    "            params = {\n",
    "                'objective': 'rank:ndcg',\n",
    "                'eval_metric': 'ndcg@100',\n",
    "                'learning_rate': 0.05,\n",
    "                'ndcg_exp_gain': False,\n",
    "                'max_depth': 8,\n",
    "                'min_child_weight': 50,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'n_estimators': 300,\n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 42,\n",
    "            }\n",
    "        \n",
    "        self.logger.info(f\"Starting training with parameters: {params}\")\n",
    "        self.logger.info(f\"Using {n_folds}-fold cross-validation\")\n",
    "        \n",
    "        # Get unique groups to ensure they're not split across folds\n",
    "        unique_groups = np.unique(groups)\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=params.get('random_state', 42))\n",
    "        \n",
    "        fold_scores = []\n",
    "        self.models = []\n",
    "        \n",
    "        # For tracking best model\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(unique_groups), \n",
    "                                                        total=n_folds, \n",
    "                                                        desc=\"Cross-validation\")):\n",
    "            self.logger.info(f\"\\n{'='*50}\\nFold {fold+1}/{n_folds}\\n{'='*50}\")\n",
    "            \n",
    "            # Get indices for this fold based on groups\n",
    "            train_groups = unique_groups[train_idx]\n",
    "            val_groups = unique_groups[val_idx]\n",
    "            \n",
    "            train_mask = np.isin(groups, train_groups)\n",
    "            val_mask = np.isin(groups, val_groups)\n",
    "            \n",
    "            X_train, y_train, groups_train = X[train_mask], y[train_mask], groups[train_mask]\n",
    "            X_val, y_val, groups_val = X[val_mask], y[val_mask], groups[val_mask]\n",
    "            \n",
    "            # Create DMatrix objects\n",
    "            dtrain = self._create_dmatrix(X_train, y_train, groups_train)\n",
    "            dval = self._create_dmatrix(X_val, y_val, groups_val)\n",
    "            \n",
    "            # Setup watch list for monitoring progress\n",
    "            watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "            \n",
    "            # Train the model for this fold\n",
    "            evals_result = {}\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=params.get('n_estimators', 300),\n",
    "                evals=watchlist,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=10 if self.verbose else False\n",
    "            )\n",
    "            \n",
    "            # Save the model and validation score\n",
    "            self.models.append(model)\n",
    "            best_iteration = model.best_iteration\n",
    "            best_score_fold = max(evals_result['validation'][params['eval_metric']])\n",
    "            fold_scores.append(best_score_fold)\n",
    "            \n",
    "            self.logger.info(f\"Fold {fold+1} best score: {best_score_fold:.6f} at iteration {best_iteration}\")\n",
    "            \n",
    "            # Track the best model\n",
    "            if best_score_fold > best_score:\n",
    "                best_score = best_score_fold\n",
    "                best_model = model\n",
    "                \n",
    "        # Set the best model\n",
    "        self.best_model = best_model\n",
    "        \n",
    "        # Calculate feature importances from all models\n",
    "        if best_model is not None:\n",
    "            self.feature_importances = best_model.get_score(importance_type='gain')\n",
    "        \n",
    "        mean_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(f\"\\n{'='*50}\")\n",
    "        self.logger.info(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        self.logger.info(f\"Cross-validation scores: {fold_scores}\")\n",
    "        self.logger.info(f\"Mean CV score: {mean_score:.6f} ± {std_score:.6f}\")\n",
    "        self.logger.info(f\"Best model score: {best_score:.6f}\")\n",
    "        \n",
    "        if self.feature_importances:\n",
    "            self.logger.info(\"Top 10 feature importances:\")\n",
    "            sorted_features = sorted(self.feature_importances.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            for feature, importance in sorted_features:\n",
    "                self.logger.info(f\"  {feature}: {importance:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, use_all_models: bool = False) -> np.ndarray:\n",
    "        start_time = time.time()\n",
    "        self.logger.info(f\"Generating predictions for {X.shape[0]} samples...\")\n",
    "        \n",
    "        dtest = xgb.DMatrix(X)\n",
    "        \n",
    "        if use_all_models and len(self.models) > 0:\n",
    "            # Ensemble prediction by averaging all models\n",
    "            self.logger.info(\"Using ensemble prediction from all folds\")\n",
    "            preds = np.zeros(X.shape[0])\n",
    "            for model in self.models:\n",
    "                preds += model.predict(dtest)\n",
    "            preds /= len(self.models)\n",
    "        elif self.best_model is not None:\n",
    "            # Use only the best model\n",
    "            self.logger.info(\"Using best model for prediction\")\n",
    "            preds = self.best_model.predict(dtest)\n",
    "        else:\n",
    "            raise ValueError(\"No trained models available. Please fit the model first.\")\n",
    "        \n",
    "        prediction_time = time.time() - start_time\n",
    "        self.logger.info(f\"Prediction completed in {prediction_time:.2f} seconds\")\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def get_feature_importances(self) -> Dict[str, float]:\n",
    "        if self.feature_importances is None:\n",
    "            self.logger.warning(\"Feature importances not available. Model may not be trained.\")\n",
    "            return {}\n",
    "        return self.feature_importances\n",
    "    \n",
    "    def save_model(self, filepath: str, save_all: bool = False) -> None:\n",
    "        if self.best_model is not None:\n",
    "            best_model_path = f\"{filepath}_best.json\"\n",
    "            self.best_model.save_model(best_model_path)\n",
    "            self.logger.info(f\"Best model saved to {best_model_path}\")\n",
    "            \n",
    "        if save_all and len(self.models) > 0:\n",
    "            for i, model in enumerate(self.models):\n",
    "                fold_path = f\"{filepath}_fold_{i}.json\"\n",
    "                model.save_model(fold_path)\n",
    "            self.logger.info(f\"All {len(self.models)} fold models saved\")\n",
    "    \n",
    "    def load_model(self, filepath: str):\n",
    "        self.best_model = xgb.Booster()\n",
    "        self.best_model.load_model(filepath)\n",
    "        self.logger.info(f\"Model loaded from {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            self.feature_importances = self.best_model.get_score(importance_type='gain')\n",
    "        except:\n",
    "            self.logger.warning(\"Could not extract feature importances from loaded model\")\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528892b-cc50-4395-a140-ebecf8fabaa6",
   "metadata": {},
   "source": [
    "#### Loading Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cf9b7e-add1-4c17-88d7-4c97c6cc1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10300969 entries, 0 to 10300968\n",
      "Data columns (total 41 columns):\n",
      " #   Column                              Dtype  \n",
      "---  ------                              -----  \n",
      " 0   video_id                            int16  \n",
      " 1   user_id                             int16  \n",
      " 2   engagement                          float32\n",
      " 3   like_play_ratio                     float32\n",
      " 4   comment_play_ratio                  float32\n",
      " 5   share_play_ratio                    float32\n",
      " 6   like_cancel_ratio                   float32\n",
      " 7   video_duration                      float32\n",
      " 8   is_add                              int16  \n",
      " 9   like_to_comment_ratio               float32\n",
      " 10  follow_play_ratio                   float32\n",
      " 11  follow_cancel_ratio                 float32\n",
      " 12  is_lowactive_period                 int16  \n",
      " 13  is_live_streamer                    int16  \n",
      " 14  is_video_author                     int16  \n",
      " 15  follower_fan_ratio                  float32\n",
      " 16  popularity_score                    float32\n",
      " 17  video_id_count                      int16  \n",
      " 18  is_user_active                      int16  \n",
      " 19  preferred_duration                  float32\n",
      " 20  category_match                      int16  \n",
      " 21  upload_type_match                   int16  \n",
      " 22  duration_diff                       float32\n",
      " 23  upload_type_AiCutVideo              int16  \n",
      " 24  upload_type_FlashPhoto              int16  \n",
      " 25  upload_type_FollowShoot             int16  \n",
      " 26  upload_type_Kmovie                  int16  \n",
      " 27  upload_type_LocalCollection         int16  \n",
      " 28  upload_type_LocalIntelligenceAlbum  int16  \n",
      " 29  upload_type_LongCamera              int16  \n",
      " 30  upload_type_LongImport              int16  \n",
      " 31  upload_type_LongPicture             int16  \n",
      " 32  upload_type_PhotoCopy               int16  \n",
      " 33  upload_type_PictureCopy             int16  \n",
      " 34  upload_type_PictureSet              int16  \n",
      " 35  upload_type_SameFrame               int16  \n",
      " 36  upload_type_ShareFromOtherApp       int16  \n",
      " 37  upload_type_ShortCamera             int16  \n",
      " 38  upload_type_ShortImport             int16  \n",
      " 39  upload_type_UNKNOWN                 int16  \n",
      " 40  upload_type_Web                     int16  \n",
      "dtypes: float32(13), int16(28)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_parquet(\"dataset/train_processed_data.parquet\")\n",
    "df_train.info(memory_usage=\"deep\")\n",
    "X_cols = [col for col in df_train.columns if col != \"engagement\"]\n",
    "X_train = df_train[X_cols].to_numpy(copy=False)\n",
    "y_train = np.ceil(df_train[\"engagement\"].to_numpy(copy=False)).astype(int)\n",
    "groups_train = df_train[\"user_id\"].to_numpy(copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c43a3f-cc44-404b-9c35-cc22e2171681",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d496b4-876c-4e0e-b9e1-fd73f3c8b299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1309\n"
     ]
    }
   ],
   "source": [
    "print(y_train.min())\n",
    "print(y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893eab1-0794-473e-97b5-38a466e309fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:26:11,301 - XGRanker - INFO - Starting training with parameters: {'objective': 'rank:ndcg', 'eval_metric': 'ndcg@100', 'learning_rate': 0.05, 'ndcg_exp_gain': False, 'max_depth': 8, 'min_child_weight': 50, 'subsample': 0.8, 'colsample_bytree': 0.7, 'n_estimators': 300, 'tree_method': 'hist', 'random_state': 42}\n",
      "2025-05-05 14:26:11,302 - XGRanker - INFO - Using 3-fold cross-validation\n",
      "Cross-validation:   0%|          | 0/3 [00:00<?, ?it/s]2025-05-05 14:26:11,480 - XGRanker - INFO - \n",
      "==================================================\n",
      "Fold 1/3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model = ContentBasedFilteringXGRanker()\n",
    "model.fit(X_train, y_train, groups_train, params={\n",
    "    'objective': 'rank:ndcg',\n",
    "    'eval_metric': 'ndcg@100',\n",
    "    'learning_rate': 0.05,\n",
    "    'ndcg_exp_gain': False,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 50,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'n_estimators': 300,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7fb45-7cd9-40cb-a000-89f0da93c494",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959335ec-197d-4bfa-8d2b-360239e3bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(\"dataset/test_processed_data.parquet\")\n",
    "\n",
    "X_test = df_train[list(set(df_train.columns) - set([\"engagement\"]))].to_numpy()\n",
    "y_test = df_train[\"engagement\"].to_numpy()\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d03714-e6ad-443e-8695-29486402f1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9250729 , 0.94992095, 0.93757343, ..., 0.9539452 , 0.97146255,\n",
       "       0.93965286], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e40c6-610c-4f95-99c9-5e05a3b70f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32358b0a-10da-47f4-9861-d55fa711469a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d07f2b-cb28-4993-8d07-5331f943154f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff15ece-111f-4170-b61a-07b130190964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
